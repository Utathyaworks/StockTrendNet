{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10644584,"sourceType":"datasetVersion","datasetId":6591013},{"sourceId":10670161,"sourceType":"datasetVersion","datasetId":6608730},{"sourceId":10681944,"sourceType":"datasetVersion","datasetId":6617585},{"sourceId":10701073,"sourceType":"datasetVersion","datasetId":6631576},{"sourceId":10708519,"sourceType":"datasetVersion","datasetId":6631289},{"sourceId":10721662,"sourceType":"datasetVersion","datasetId":6646244},{"sourceId":10921766,"sourceType":"datasetVersion","datasetId":6790193},{"sourceId":10921810,"sourceType":"datasetVersion","datasetId":6790221},{"sourceId":10992211,"sourceType":"datasetVersion","datasetId":6631332},{"sourceId":12163631,"sourceType":"datasetVersion","datasetId":7660804},{"sourceId":12163661,"sourceType":"datasetVersion","datasetId":7660815}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:10:09.187249Z","iopub.execute_input":"2025-06-14T15:10:09.187732Z","iopub.status.idle":"2025-06-14T15:10:09.712011Z","shell.execute_reply.started":"2025-06-14T15:10:09.187689Z","shell.execute_reply":"2025-06-14T15:10:09.710682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Paper 2\n* This paper does prediction on BSE SENSEX data\n* Data spans the dates of May 30, 2010, and February 9, 2018\n* It uses RMSE and MSE metric\n* Their best result is MSE-0.0021 RMSE-0.0438\n* Paper link:-https://ieeexplore.ieee.org/document/10397684","metadata":{}},{"cell_type":"markdown","source":"## Trying without vix","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load Sensex dataset\ndata = pd.read_csv(\"/kaggle/input/d/abirc8010/historical-india-stock-market/BSE Sensex 30 Historical Data.csv\")\n\n# Print initial number of rows\ninitial_count = len(data)\nprint(f\"Initial number of rows: {initial_count}\")\n\n# Reverse and reset index\ndata = data[::-1].reset_index(drop=True)\n\n# Sort columns alphabetically and rename 'Price' to 'Close'\ndata.sort_index(axis=1, ascending=True, inplace=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\n\n# Create a copy to avoid modifying the original data\ndf = data.copy()\n\n# Convert financial columns to numeric\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):\n        vol = vol.replace(\",\", \"\")\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Convert 'Date' column to datetime format, handling mixed formats\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True, errors=\"coerce\")\n\n# Filter Sensex data within the specified date range\nstart_date = \"2010-05-30\"\nend_date = \"2018-02-09\"\ndf = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)]\n\n# Print final number of rows\nfinal_count = len(df)\nprint(f\"Final number of rows after filtering: {final_count}\")\n\n# Assign back to 'data'\ndata = df\n\n# Drop 'Date' column\ndata.drop([\"Date\"], axis=1, inplace=True)\n\n# Print final dataset\nprint(data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:10:09.713526Z","iopub.execute_input":"2025-06-14T15:10:09.714049Z","iopub.status.idle":"2025-06-14T15:10:09.795030Z","shell.execute_reply.started":"2025-06-14T15:10:09.714016Z","shell.execute_reply":"2025-06-14T15:10:09.793850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(30)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"📊 Final Results -\")\nprint(f\"  R²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"R² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:10:09.797407Z","iopub.execute_input":"2025-06-14T15:10:09.797851Z","iopub.status.idle":"2025-06-14T15:11:00.751135Z","shell.execute_reply.started":"2025-06-14T15:10:09.797820Z","shell.execute_reply":"2025-06-14T15:11:00.749648Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Paper 3\n* This paper does prediction on Nifty50 , Sensex and S&P 500\n* The data is collected daily from 2013 to 2022\n* The metric used is RMSE\n* Their result is NIFTY50:-RMSE - 170.843,SENSEX:-RMSE - 578.746 S&P 500:-RMSE -  50.1650\n* Paper link:-https://www.sciencedirect.com/science/article/pii/S1568494624005337","metadata":{}},{"cell_type":"markdown","source":"## Doing without vix","metadata":{}},{"cell_type":"markdown","source":"## Working with Sensex","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/personal-dataset/Sensex Paper1.csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n#data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:11:00.753515Z","iopub.execute_input":"2025-06-14T15:11:00.754113Z","iopub.status.idle":"2025-06-14T15:11:00.819858Z","shell.execute_reply.started":"2025-06-14T15:11:00.754078Z","shell.execute_reply":"2025-06-14T15:11:00.818903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(30)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"📊 Final Results -\")\nprint(f\"  R²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"R² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:11:00.820912Z","iopub.execute_input":"2025-06-14T15:11:00.821195Z","iopub.status.idle":"2025-06-14T15:11:24.103208Z","shell.execute_reply.started":"2025-06-14T15:11:00.821170Z","shell.execute_reply":"2025-06-14T15:11:24.101779Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Nifty50","metadata":{}},{"cell_type":"markdown","source":"## doing without vix","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty50 Paper1.csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndata['Vol.']=data['Vol.'].fillna(\"278.04M\")\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n#data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:11:24.104516Z","iopub.execute_input":"2025-06-14T15:11:24.104986Z","iopub.status.idle":"2025-06-14T15:11:24.193792Z","shell.execute_reply.started":"2025-06-14T15:11:24.104952Z","shell.execute_reply":"2025-06-14T15:11:24.192373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(feature_cols))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:11:24.194995Z","iopub.execute_input":"2025-06-14T15:11:24.195479Z","iopub.status.idle":"2025-06-14T15:11:24.202738Z","shell.execute_reply.started":"2025-06-14T15:11:24.195401Z","shell.execute_reply":"2025-06-14T15:11:24.200895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(30)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"📊 Final Results -\")\nprint(f\"  R²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"R² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:11:24.207150Z","iopub.execute_input":"2025-06-14T15:11:24.207728Z","iopub.status.idle":"2025-06-14T15:11:46.094117Z","shell.execute_reply.started":"2025-06-14T15:11:24.207695Z","shell.execute_reply":"2025-06-14T15:11:46.092575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with S&P500","metadata":{}},{"cell_type":"markdown","source":"## Doing without vix","metadata":{}},{"cell_type":"code","source":"\n\n# Load the CSV file\ndf = pd.read_csv(\"/kaggle/input/personal-dateset-vix/SP 500 (2).csv\")\n\n# Ensure the required columns exist\nif \"Price\" in df.columns and \"Open\" in df.columns:\n    # Calculate the percentage change\n    df[\"Change %\"] = ((df[\"Price\"] - df[\"Open\"]) / df[\"Price\"]) * 100\n\n    # Save the updated CSV file\n    # df.to_csv(\"SP 500 (2).csv\", index=False)\n\n    # Display first few rows\n    print(df.head())\nelse:\n    print(\"Error: CSV file must contain 'Price' and 'Open' columns.\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:11:46.095884Z","iopub.execute_input":"2025-06-14T15:11:46.096253Z","iopub.status.idle":"2025-06-14T15:11:46.126211Z","shell.execute_reply.started":"2025-06-14T15:11:46.096223Z","shell.execute_reply":"2025-06-14T15:11:46.125307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=df\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\nprint(data.head())\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\n#df.drop(\"Vol.\",axis=1,inplace=True)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n#data['Date'] = pd.to_datetime(data['Date'], format=\"%d-%m-%Y\")\n\n# For 'vix', the date format is \"MM/DD/YYYY\"\n#vix['Date'] = pd.to_datetime(vix['Date'], format=\"%m/%d/%Y\")\n#data = pd.merge(data, vix, on='Date', how='inner')\n#row=data[data[\"Vol.\"].isna()]\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:11:46.127186Z","iopub.execute_input":"2025-06-14T15:11:46.127494Z","iopub.status.idle":"2025-06-14T15:11:46.184299Z","shell.execute_reply.started":"2025-06-14T15:11:46.127436Z","shell.execute_reply":"2025-06-14T15:11:46.183294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(20)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\ny_test_inv3, y_pred_inv3 = true_values_inv, predictions_inv\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"📊 Final Results -\")\nprint(f\"  R²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"R² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:11:46.185694Z","iopub.execute_input":"2025-06-14T15:11:46.186001Z","iopub.status.idle":"2025-06-14T15:13:43.618611Z","shell.execute_reply.started":"2025-06-14T15:11:46.185974Z","shell.execute_reply":"2025-06-14T15:13:43.617274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Paper 4\n* This paper does prediction on Nifty50\n* The metric used is RMSE\n* Their result is RMSE - 171.4\n* We are training the model on data from 1st January 2020 till 31st December 2023 and testing it on data from 1st January\n  2024 till 15th March 2024.\n* Paper link:-https://www.semanticscholar.org/paper/Closing-Price-Prediction-for-the-NIFTY-50-Index%3A-A-Singh-Shah/d0eca144e2cd8e86c57eca34e3d9f4943f3c45f2","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndata=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Train paper 2.csv\")\ndata2=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Test paper2.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:13:43.619816Z","iopub.execute_input":"2025-06-14T15:13:43.620250Z","iopub.status.idle":"2025-06-14T15:13:43.642432Z","shell.execute_reply.started":"2025-06-14T15:13:43.620220Z","shell.execute_reply":"2025-06-14T15:13:43.641132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head(),data2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:13:43.643531Z","iopub.execute_input":"2025-06-14T15:13:43.643938Z","iopub.status.idle":"2025-06-14T15:13:43.656991Z","shell.execute_reply.started":"2025-06-14T15:13:43.643909Z","shell.execute_reply":"2025-06-14T15:13:43.655681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(data):\n    data=data[::-1]\n    data.reset_index(drop=True, inplace=True)\n    print(data.head())\n    data.nunique()\n\n    data.sort_index(axis=1,ascending=True)\n    data['Vol.']=data['Vol.'].fillna(\"278.04M\")\n    df=data\n    print(df)\n    data.rename(columns={'Price': 'Close'}, inplace=True)\n    print(data.head()),print(data.dtypes)\n    data['Vol.']=data['Vol.'].fillna(\"278.04M\")\n    df = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\n    for col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n        df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\n    def convert_volume(vol):\n        if isinstance(vol, str):  # Ensure it's a string before replacing\n            vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n            if \"B\" in vol:\n                return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n            elif \"M\" in vol:\n                return float(vol.replace(\"M\", \"\")) * 1_000_000\n            elif \"K\" in vol:\n                return float(vol.replace(\"K\", \"\")) * 1_000\n        return float(vol)  # Convert directly if already a number\n\n    df[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\n    df[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\n    print(df.dtypes)\n    print(df)\n\n# Assign back to 'data' (if needed)\n    data = df\n    return data\n    #data=df\n#data.drop(\"Date\",axis=1,inplace=True)\n\ndata=preprocess(data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:13:43.658298Z","iopub.execute_input":"2025-06-14T15:13:43.658654Z","iopub.status.idle":"2025-06-14T15:13:43.716266Z","shell.execute_reply.started":"2025-06-14T15:13:43.658627Z","shell.execute_reply":"2025-06-14T15:13:43.715087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2=preprocess(data2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:13:43.717331Z","iopub.execute_input":"2025-06-14T15:13:43.717747Z","iopub.status.idle":"2025-06-14T15:13:43.758542Z","shell.execute_reply.started":"2025-06-14T15:13:43.717718Z","shell.execute_reply":"2025-06-14T15:13:43.757181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vix = pd.read_csv(\"/kaggle/input/vix-paper2/vix paper 2.csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:13:43.759661Z","iopub.execute_input":"2025-06-14T15:13:43.760035Z","iopub.status.idle":"2025-06-14T15:13:43.805402Z","shell.execute_reply.started":"2025-06-14T15:13:43.759992Z","shell.execute_reply":"2025-06-14T15:13:43.804205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(\"Date\",axis=1,inplace=True)\nprint(data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:13:43.806557Z","iopub.execute_input":"2025-06-14T15:13:43.806853Z","iopub.status.idle":"2025-06-14T15:13:43.827386Z","shell.execute_reply.started":"2025-06-14T15:13:43.806823Z","shell.execute_reply":"2025-06-14T15:13:43.826211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2 = pd.merge(data2, vix, on='Date', how='inner')\ndata2.drop(\"Date\",axis=1,inplace=True)\nprint(data2.head())\nprint(len(data2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:13:43.828596Z","iopub.execute_input":"2025-06-14T15:13:43.829027Z","iopub.status.idle":"2025-06-14T15:13:43.898910Z","shell.execute_reply.started":"2025-06-14T15:13:43.828986Z","shell.execute_reply":"2025-06-14T15:13:43.897417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## doing without vix","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Train paper 2.csv\")\ndata2=pd.read_csv(\"/kaggle/input/personal-dataset/Nifty 50 Test paper2.csv\")\ndata=preprocess(data)\ndata2=preprocess(data2)\ndata2.drop(\"Date\",axis=1,inplace=True)\ndata.drop(\"Date\",axis=1,inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:13:43.900352Z","iopub.execute_input":"2025-06-14T15:13:43.900856Z","iopub.status.idle":"2025-06-14T15:13:43.988912Z","shell.execute_reply.started":"2025-06-14T15:13:43.900812Z","shell.execute_reply":"2025-06-14T15:13:43.987606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(30)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"📊 Final Results -\")\nprint(f\"  R²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"R² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:13:43.990255Z","iopub.execute_input":"2025-06-14T15:13:43.990708Z","iopub.status.idle":"2025-06-14T15:13:54.979890Z","shell.execute_reply.started":"2025-06-14T15:13:43.990668Z","shell.execute_reply":"2025-06-14T15:13:54.978358Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Paper 5\n* This paper does prediction on Nifty50 and BSE Sensex\n* The ranges are:-\n* ## BSE( Data )\n 24/02/2015-27/11/2022\n\n* ## NSE(Data)\n 27/10/2006-27/11/2022\n* The metric used are RMSE,MSE,MAE\n* Their best results are:-\n* ## BSE 15 Day:\nMAE Scaled: 0.0522\n MSE Scaled: 0.0515\n RMSE Scaled: 0.1456\n* ## NSE 15 Day:\nMAE Scaled: 0.0712\n MSE Scaled: 0.0521\n  RMSE Scaled: 0.1365\n* Paper link:-https://ieeexplore.ieee.org/document/10481618","metadata":{}},{"cell_type":"markdown","source":"### Sensex (without vix)","metadata":{}},{"cell_type":"code","source":"\ndata=pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/BSE Sensex 30 Historical Data ( paper - 5 ).csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n#data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:13:54.981225Z","iopub.execute_input":"2025-06-14T15:13:54.981962Z","iopub.status.idle":"2025-06-14T15:13:55.054978Z","shell.execute_reply.started":"2025-06-14T15:13:54.981921Z","shell.execute_reply":"2025-06-14T15:13:55.053165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(30)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"📊 Final Results -\")\nprint(f\"  R²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"R² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:13:55.056505Z","iopub.execute_input":"2025-06-14T15:13:55.056962Z","iopub.status.idle":"2025-06-14T15:14:25.530957Z","shell.execute_reply.started":"2025-06-14T15:13:55.056910Z","shell.execute_reply":"2025-06-14T15:14:25.529797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total Parameters: {total_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:14:25.535501Z","iopub.execute_input":"2025-06-14T15:14:25.535841Z","iopub.status.idle":"2025-06-14T15:14:25.541479Z","shell.execute_reply.started":"2025-06-14T15:14:25.535816Z","shell.execute_reply":"2025-06-14T15:14:25.540302Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Nifty50 without vix","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/paper-5-deepstock-dataset/Nifty 50 Historical Data - paper( 5 ).csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True) \ndata['Vol.']=data['Vol.'].fillna(\"278.04M\")\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n#data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:14:25.543990Z","iopub.execute_input":"2025-06-14T15:14:25.544383Z","iopub.status.idle":"2025-06-14T15:14:25.642927Z","shell.execute_reply.started":"2025-06-14T15:14:25.544352Z","shell.execute_reply":"2025-06-14T15:14:25.641933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(30)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"📊 Final Results -\")\nprint(f\"  R²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"R² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:14:25.644113Z","iopub.execute_input":"2025-06-14T15:14:25.644465Z","iopub.status.idle":"2025-06-14T15:14:57.616547Z","shell.execute_reply.started":"2025-06-14T15:14:25.644414Z","shell.execute_reply":"2025-06-14T15:14:57.614660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total Parameters: {total_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:14:57.618011Z","iopub.execute_input":"2025-06-14T15:14:57.618579Z","iopub.status.idle":"2025-06-14T15:14:57.628077Z","shell.execute_reply.started":"2025-06-14T15:14:57.618525Z","shell.execute_reply":"2025-06-14T15:14:57.626558Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OUR RANGE: SENSEX (15-3-2008 to 15-3-2024)","metadata":{}},{"cell_type":"code","source":"vix = pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/India VIX Historical Data (3).csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes\ndata=pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/BSE Sensex 30 Historical Data (3).csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n# data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:14:57.629251Z","iopub.execute_input":"2025-06-14T15:14:57.629783Z","iopub.status.idle":"2025-06-14T15:14:57.754944Z","shell.execute_reply.started":"2025-06-14T15:14:57.629749Z","shell.execute_reply":"2025-06-14T15:14:57.753196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(30)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"📊 Final Results -\")\nprint(f\"  R²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"R² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:14:57.756685Z","iopub.execute_input":"2025-06-14T15:14:57.757130Z","iopub.status.idle":"2025-06-14T15:15:31.552911Z","shell.execute_reply.started":"2025-06-14T15:14:57.757086Z","shell.execute_reply":"2025-06-14T15:15:31.551556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total Parameters: {total_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:15:31.553985Z","iopub.execute_input":"2025-06-14T15:15:31.554564Z","iopub.status.idle":"2025-06-14T15:15:31.560905Z","shell.execute_reply.started":"2025-06-14T15:15:31.554526Z","shell.execute_reply":"2025-06-14T15:15:31.559604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OUR RANGE: NIFTY50 (15-3-2008 to 15-3-2024)","metadata":{}},{"cell_type":"code","source":"vix = pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/India VIX Historical Data (3).csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes\ndata=pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/Nifty 50 Historical Data (2).csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndata['Vol.']=data['Vol.'].fillna(\"278.04M\")\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n#data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:15:31.561926Z","iopub.execute_input":"2025-06-14T15:15:31.562245Z","iopub.status.idle":"2025-06-14T15:15:31.679965Z","shell.execute_reply.started":"2025-06-14T15:15:31.562219Z","shell.execute_reply":"2025-06-14T15:15:31.678604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(30)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"📊 Final Results -\")\nprint(f\"  R²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"R² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:15:31.681185Z","iopub.execute_input":"2025-06-14T15:15:31.681581Z","iopub.status.idle":"2025-06-14T15:16:11.330837Z","shell.execute_reply.started":"2025-06-14T15:15:31.681551Z","shell.execute_reply":"2025-06-14T15:16:11.329676Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OUR RANGE: S&P 500 (15-3-2008 to 15-3-2024)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv(\"/kaggle/input/sp500-dataset/sp500_historical_data (2).csv\")\n\n# Ensure the required columns exist\nif \"Price\" in df.columns and \"Open\" in df.columns:\n    # Calculate the percentage change\n    df[\"Change %\"] = ((df[\"Price\"] - df[\"Open\"]) / df[\"Price\"]) * 100\n\n    # Save the updated CSV file\n    # df.to_csv(\"SP 500 (2).csv\", index=False)\n\n    # Display first few rows\n    print(df.head())\nelse:\n    print(\"Error: CSV file must contain 'Price' and 'Open' columns.\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:16:11.331975Z","iopub.execute_input":"2025-06-14T15:16:11.332394Z","iopub.status.idle":"2025-06-14T15:16:11.374842Z","shell.execute_reply.started":"2025-06-14T15:16:11.332360Z","shell.execute_reply":"2025-06-14T15:16:11.373329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=df\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\nprint(data.head())\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\n#df.drop(\"Vol.\",axis=1,inplace=True)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n#data['Date'] = pd.to_datetime(data['Date'], format=\"%d-%m-%Y\")\n\n# For 'vix', the date format is \"MM/DD/YYYY\"\n#vix['Date'] = pd.to_datetime(vix['Date'], format=\"%m/%d/%Y\")\n#data = pd.merge(data, vix, on='Date', how='inner')\n#row=data[data[\"Vol.\"].isna()]\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:16:11.376267Z","iopub.execute_input":"2025-06-14T15:16:11.376682Z","iopub.status.idle":"2025-06-14T15:16:11.447741Z","shell.execute_reply.started":"2025-06-14T15:16:11.376651Z","shell.execute_reply":"2025-06-14T15:16:11.446715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(30)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\ny_test_inv3, y_pred_inv3 = true_values_inv, predictions_inv\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"📊 Final Results -\")\nprint(f\"  R²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"R² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:16:11.448676Z","iopub.execute_input":"2025-06-14T15:16:11.448983Z","iopub.status.idle":"2025-06-14T15:18:19.800962Z","shell.execute_reply.started":"2025-06-14T15:16:11.448957Z","shell.execute_reply":"2025-06-14T15:18:19.799394Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OUR RANGE: NIKKIE225 (15-3-2008 to 15-3-2024)","metadata":{}},{"cell_type":"code","source":"'''vix = pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/India VIX Historical Data (3).csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes'''\ndata=pd.read_csv(\"/kaggle/input/nikkie/Nikkei 225 Historical Data.csv\")\ndisplay(data)\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndata.drop([\"Vol.\"], axis=1, inplace=True)\ndf = data.copy()  # Ensure we don't modify the original dataset\n\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\n'''\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n'''\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n# data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:18:19.802363Z","iopub.execute_input":"2025-06-14T15:18:19.802876Z","iopub.status.idle":"2025-06-14T15:18:19.889612Z","shell.execute_reply.started":"2025-06-14T15:18:19.802830Z","shell.execute_reply":"2025-06-14T15:18:19.888529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(30)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\ny_test_inv4, y_pred_inv4 = true_values_inv, predictions_inv\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"📊 Final Results -\")\nprint(f\"  R²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"R² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:18:19.890656Z","iopub.execute_input":"2025-06-14T15:18:19.890925Z","iopub.status.idle":"2025-06-14T15:18:59.186923Z","shell.execute_reply.started":"2025-06-14T15:18:19.890903Z","shell.execute_reply":"2025-06-14T15:18:59.185681Z"}},"outputs":[],"execution_count":null}]}