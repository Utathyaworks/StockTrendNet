{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10644584,"sourceType":"datasetVersion","datasetId":6591013},{"sourceId":10670161,"sourceType":"datasetVersion","datasetId":6608730},{"sourceId":10681944,"sourceType":"datasetVersion","datasetId":6617585},{"sourceId":10701073,"sourceType":"datasetVersion","datasetId":6631576},{"sourceId":10708519,"sourceType":"datasetVersion","datasetId":6631289},{"sourceId":10721662,"sourceType":"datasetVersion","datasetId":6646244},{"sourceId":10921766,"sourceType":"datasetVersion","datasetId":6790193},{"sourceId":10921810,"sourceType":"datasetVersion","datasetId":6790221},{"sourceId":10992211,"sourceType":"datasetVersion","datasetId":6631332}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:17:29.547398Z","iopub.execute_input":"2025-06-14T13:17:29.547632Z","iopub.status.idle":"2025-06-14T13:17:29.980731Z","shell.execute_reply.started":"2025-06-14T13:17:29.547600Z","shell.execute_reply":"2025-06-14T13:17:29.979873Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OUR RANGE: NIFTY50 (15-3-2008 to 15-3-2024)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nvix = pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/India VIX Historical Data (3).csv\")\nvix.drop(\"Vol.\",axis=1,inplace=True)\nvix[\"Change %\"] = vix[\"Change %\"].str.replace(\"%\", \"\").astype(float)\nvix.head(),vix.dtypes\ndata=pd.read_csv(\"/kaggle/input/our-data-15-3-2008-to-15-3-2024/Nifty 50 Historical Data (2).csv\")\ndata=data[::-1]\ndata.reset_index(drop=True, inplace=True)\ndata.head()\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\ndata.rename(columns={'Price': 'Close'}, inplace=True)\ndata['Vol.']=data['Vol.'].fillna(\"278.04M\")\ndf = data.copy()  # Ensure we don't modify the original datase\n# Convert financial columns to numeric (remove commas)\nfor col in [\"Close\", \"Open\", \"High\", \"Low\"]:\n    df[col] = df[col].astype(str).str.replace(\",\", \"\").astype(float)\n\n# Function to convert 'Vol.' column\ndef convert_volume(vol):\n    if isinstance(vol, str):  # Ensure it's a string before replacing\n        vol = vol.replace(\",\", \"\")  # Remove any thousand separators\n        if \"B\" in vol:\n            return float(vol.replace(\"B\", \"\")) * 1_000_000_000\n        elif \"M\" in vol:\n            return float(vol.replace(\"M\", \"\")) * 1_000_000\n        elif \"K\" in vol:\n            return float(vol.replace(\"K\", \"\")) * 1_000\n    return float(vol)  # Convert directly if already a number\n\ndf[\"Vol.\"] = df[\"Vol.\"].astype(str).apply(convert_volume)\n\n# Convert 'Change %' column (remove '%' and convert to float)\ndf[\"Change %\"] = df[\"Change %\"].astype(str).str.replace(\"%\", \"\").astype(float)\n\n# Print final DataFrame\nprint(df.dtypes)\nprint(df)\n\n# Assign back to 'data' (if needed)\ndata = df\n\n#data = pd.merge(data, vix, on='Date', how='inner')\ndata.drop(['Date'], axis=1, inplace=True)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:17:29.981937Z","iopub.execute_input":"2025-06-14T13:17:29.982396Z","iopub.status.idle":"2025-06-14T13:17:30.066144Z","shell.execute_reply.started":"2025-06-14T13:17:29.982369Z","shell.execute_reply":"2025-06-14T13:17:30.064830Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## With Single LSTM Model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.metrics import (\n    r2_score,\n    mean_absolute_error,\n    mean_squared_error,\n    explained_variance_score,\n    mean_absolute_percentage_error\n)\nimport matplotlib.pyplot as plt\n\n# 1. Seed for reproducibility\ndef set_seed(seed=30):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(26)\n\n# 2. Sequence creation\ndef create_sequences(arr, seq_length=20, target_idx=0):\n    X, y = [], []\n    for i in range(len(arr) - seq_length):\n        X.append(arr[i : i + seq_length])\n        y.append(arr[i + seq_length, target_idx])\n    return np.array(X), np.array(y)\n\n# 3. Train/Test split + scaling\nvalues = data.values  # your preprocessed DataFrame\ntrain_size = int(len(values) * 0.8)\ntrain_data = values[:train_size]\ntest_data  = values[train_size:]\n\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# 4. Create sequences\nseq_length = 20\nX_train_full, y_train_full = create_sequences(train_scaled, seq_length, target_idx=0)\nX_test,        y_test        = create_sequences(test_scaled,  seq_length, target_idx=0)\n\n# 5. Further split train â†’ train/val (87.5%/12.5%)\nsplit = int(len(X_train_full) * 0.875)\nX_train, y_train = X_train_full[:split], y_train_full[:split]\nX_val,   y_val   = X_train_full[split:], y_train_full[split:]\n\n# 6. PyTorch DataLoaders\nbatch_size = 32\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef to_loader(X, y, shuffle=False):\n    ds = TensorDataset(\n        torch.tensor(X, dtype=torch.float32),\n        torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n    )\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n\ntrain_loader = to_loader(X_train, y_train, shuffle=True)\nval_loader   = to_loader(X_val,   y_val)\ntest_loader  = to_loader(X_test,  y_test)\n\n# 7. Simple LSTM Model\n# Slightly Weakened LSTM Model\nclass SimpleLSTMModel(nn.Module):\n    def __init__(self, in_channels, hidden_dim=32, dense_dim=16, scale_factor=0.25):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=in_channels,\n                            hidden_size=hidden_dim,\n                            batch_first=True)\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim)\n            # No ReLU here for a weaker transformation\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        h = out[:, -1, :]\n        d = self.dense(h)\n        ret = torch.tanh(self.return_layer(d)) * self.scale_factor\n        last_close = x[:, -1, 0].unsqueeze(1)\n        return last_close * (1 + ret)\n\n# 8. Training & Early Stopping\ndef train_model(model, train_loader, val_loader,\n                lr=1e-3, epochs=100, patience=10):\n    model.to(device)\n    opt = optim.Adam(model.parameters(), lr=lr)\n    crit = nn.MSELoss()\n\n    best_val = float('inf')\n    best_state = None\n    counter = 0\n\n    for ep in range(1, epochs+1):\n        # train\n        model.train()\n        losses = []\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            opt.zero_grad()\n            out = model(Xb)\n            loss = crit(out, yb)\n            loss.backward()\n            opt.step()\n            losses.append(loss.item())\n        # val\n        model.eval()\n        vlosses = []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                vlosses.append(crit(model(Xb), yb).item())\n        train_l = np.mean(losses)\n        val_l   = np.mean(vlosses)\n        print(f\"Epoch {ep}: Train={train_l:.6f}, Val={val_l:.6f}\")\n        if val_l < best_val:\n            best_val = val_l\n            best_state = model.state_dict()\n            counter = 0\n        else:\n            counter += 1\n            if counter >= patience:\n                print(\"Early stopping\")\n                break\n\n    model.load_state_dict(best_state)\n    return model\n\n# 9. Evaluation\ndef evaluate(model, loader):\n    model.eval()\n    preds, trues = [], []\n    with torch.no_grad():\n        for Xb, yb in loader:\n            Xb = Xb.to(device)\n            out = model(Xb).cpu().numpy()\n            preds.append(out)\n            trues.append(yb.numpy())\n    preds = np.vstack(preds).flatten()\n    trues = np.vstack(trues).flatten()\n    # inverse scale\n    def inv(x):\n        tmp = np.zeros((len(x), data.shape[1]))\n        tmp[:,0] = x\n        return scaler.inverse_transform(tmp)[:,0]\n    p_inv, t_inv = inv(preds), inv(trues)\n    return {\n        'R2'   : r2_score(t_inv, p_inv),\n        'MAE'  : mean_absolute_error(t_inv, p_inv),\n        'MSE'  : mean_squared_error(t_inv, p_inv),\n        'RMSE' : np.sqrt(mean_squared_error(t_inv, p_inv)),\n        'MAPE' : mean_absolute_percentage_error(t_inv, p_inv)*100,\n        'EVS'  : explained_variance_score(t_inv, p_inv)\n    }, p_inv, t_inv\n\n# 10. Run baseline\nmodel = SimpleLSTMModel(in_channels=data.shape[1]).to(device)\nmodel = train_model(model, train_loader, val_loader)\nmetrics, preds, actuals = evaluate(model, test_loader)\n\nprint(\"Baseline LSTM Test Metrics:\")\nfor k,v in metrics.items():\n    print(f\"  {k}: {v:.4f}\")\n\n# 11. Plot\nplt.figure(figsize=(10,4))\nplt.plot(actuals, label='Actual')\nplt.plot(preds,   label='Predicted')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:17:30.068403Z","iopub.execute_input":"2025-06-14T13:17:30.068657Z","iopub.status.idle":"2025-06-14T13:17:38.058757Z","shell.execute_reply.started":"2025-06-14T13:17:30.068634Z","shell.execute_reply":"2025-06-14T13:17:38.057810Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LSTM + Update Mechanism","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.metrics import (\n    r2_score,\n    mean_absolute_error,\n    mean_squared_error,\n    explained_variance_score,\n    mean_absolute_percentage_error\n)\nimport matplotlib.pyplot as plt\n\n# 1. Seed for reproducibility\ndef set_seed(seed=30):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(30)\n\n# Load or prepare your DataFrame 'data' here\n# data = ...  # your preprocessed DataFrame with shape (n_samples, n_features)\n\n# 2. Sequence creation\ndef create_sequences(arr, seq_length=20, target_idx=0):\n    X, y = [], []\n    for i in range(len(arr) - seq_length):\n        X.append(arr[i: i + seq_length])\n        y.append(arr[i + seq_length, target_idx])\n    return np.array(X), np.array(y)\n\n# 3. Train/Test split + scaling\nvalues = data.values\ntrain_size = int(len(values) * 0.8)\ntrain_data = values[:train_size]\ntest_data  = values[train_size:]\n\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# 4. Create sequences\nseq_length = 20\nX_train_full, y_train_full = create_sequences(train_scaled, seq_length, target_idx=0)\nX_test, y_test = create_sequences(test_scaled, seq_length, target_idx=0)\n\n# 5. Further split train â†’ train/val (87.5%/12.5%)\nsplit = int(len(X_train_full) * 0.875)\nX_train, y_train = X_train_full[:split], y_train_full[:split]\nX_val, y_val     = X_train_full[split:], y_train_full[split:]\n\n# 6. PyTorch DataLoaders\nbatch_size = 64\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef to_loader(X, y, shuffle=False):\n    ds = TensorDataset(\n        torch.tensor(X, dtype=torch.float32),\n        torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n    )\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n\ntrain_loader = to_loader(X_train, y_train, shuffle=True)\nval_loader   = to_loader(X_val, y_val)\ntest_loader  = to_loader(X_test, y_test)\n\n# 7. Weakened LSTM with Update Gate Only\nclass LSTM_UpdateOnly_Weak(nn.Module):\n    def __init__(self, in_channels, hidden_dim=32, scale_factor=0.1):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.scale_factor = scale_factor\n\n        # Update gate\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n\n        # Candidate state\n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n\n        # Simplified output head\n        self.return_layer = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        batch_size, seq_length, _ = x.size()\n        h = torch.zeros(batch_size, self.hidden_dim, device=x.device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=x.device)\n\n        for t in range(seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            c = c + z_t * (candidate - c)\n            h = torch.tanh(c)\n\n        ret = torch.tanh(self.return_layer(h)) * self.scale_factor\n        last_close = x[:, -1, 0].unsqueeze(1)\n        return last_close * (1 + ret)\n\n# 8. Training & Early Stopping\ndef train_model(model, train_loader, val_loader,\n                lr=1e-3, epochs=50, patience=5):\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    best_val = float('inf')\n    best_state = None\n    counter = 0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses = []\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss.item())\n\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                val_losses.append(criterion(model(Xb), yb).item())\n\n        avg_train = np.mean(train_losses)\n        avg_val   = np.mean(val_losses)\n        print(f'Epoch {epoch}: Train={avg_train:.6f}, Val={avg_val:.6f}')\n\n        if avg_val < best_val:\n            best_val = avg_val\n            best_state = model.state_dict()\n            counter = 0\n        else:\n            counter += 1\n            if counter >= patience:\n                print('Early stopping')\n                break\n\n    model.load_state_dict(best_state)\n    return model\n\n# 9. Evaluation\ndef evaluate(model, loader):\n    model.eval()\n    preds, trues = [], []\n    with torch.no_grad():\n        for Xb, yb in loader:\n            Xb = Xb.to(device)\n            out = model(Xb).cpu().numpy()\n            preds.append(out)\n            trues.append(yb.numpy())\n    preds = np.vstack(preds).flatten()\n    trues = np.vstack(trues).flatten()\n\n    def inv(x):\n        tmp = np.zeros((len(x), data.shape[1]))\n        tmp[:,0] = x\n        return scaler.inverse_transform(tmp)[:,0]\n\n    p_inv, t_inv = inv(preds), inv(trues)\n    return {\n        'R2': r2_score(t_inv, p_inv),\n        'MAE': mean_absolute_error(t_inv, p_inv),\n        'MSE': mean_squared_error(t_inv, p_inv),\n        'RMSE': np.sqrt(mean_squared_error(t_inv, p_inv)),\n        'MAPE': mean_absolute_percentage_error(t_inv, p_inv) * 100,\n        'EVS': explained_variance_score(t_inv, p_inv)\n    }, p_inv, t_inv\n\n# 10. Run weakened model\nmodel = LSTM_UpdateOnly_Weak(in_channels=data.shape[1]).to(device)\nmodel = train_model(model, train_loader, val_loader)\nmetrics, preds, actuals = evaluate(model, test_loader)\n\nprint('Weakened Update Only LSTM Test Metrics:')\nfor k, v in metrics.items():\n    print(f'  {k}: {v:.4f}')\n\n# 11. Plot\nplt.figure(figsize=(10, 4))\nplt.plot(actuals, label='Actual')\nplt.plot(preds, label='Predicted')\nplt.legend()\nplt.title('Weakened Update Only LSTM')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:17:51.122953Z","iopub.execute_input":"2025-06-14T13:17:51.123301Z","iopub.status.idle":"2025-06-14T13:17:58.122546Z","shell.execute_reply.started":"2025-06-14T13:17:51.123273Z","shell.execute_reply":"2025-06-14T13:17:58.121608Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LSTM + Modulation Component","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.metrics import (\n    r2_score,\n    mean_absolute_error,\n    mean_squared_error,\n    explained_variance_score,\n    mean_absolute_percentage_error\n)\nimport matplotlib.pyplot as plt\n\n# 1. Seed for reproducibility\ndef set_seed(seed=30):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(28)\n\n# Load or prepare your DataFrame 'data' here\n# data = ...  # your preprocessed DataFrame with shape (n_samples, n_features)\n\n# 2. Sequence creation\ndef create_sequences(arr, seq_length=20, target_idx=0):\n    X, y = [], []\n    for i in range(len(arr) - seq_length):\n        X.append(arr[i: i + seq_length])\n        y.append(arr[i + seq_length, target_idx])\n    return np.array(X), np.array(y)\n\n# 3. Train/Test split + scaling\nvalues = data.values\ntrain_size = int(len(values) * 0.8)\ntrain_data = values[:train_size]\ntest_data  = values[train_size:]\n\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# 4. Create sequences\nseq_length = 20\nX_train_full, y_train_full = create_sequences(train_scaled, seq_length, target_idx=0)\nX_test, y_test = create_sequences(test_scaled, seq_length, target_idx=0)\n\n# 5. Further split train â†’ train/val (87.5%/12.5%)\nsplit = int(len(X_train_full) * 0.875)\nX_train, y_train = X_train_full[:split], y_train_full[:split]\nX_val, y_val     = X_train_full[split:], y_train_full[split:]\n\n# 6. PyTorch DataLoaders\nbatch_size = 64\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef to_loader(X, y, shuffle=False):\n    ds = TensorDataset(\n        torch.tensor(X, dtype=torch.float32),\n        torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n    )\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n\ntrain_loader = to_loader(X_train, y_train, shuffle=True)\nval_loader   = to_loader(X_val, y_val)\ntest_loader  = to_loader(X_test, y_test)\n\n# 7. Weakened LSTM with Modulation Gate Only\nclass LSTM_ModulationOnly_Weak(nn.Module):\n    def __init__(self, in_channels, hidden_dim=32, scale_factor=0.1):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.scale_factor = scale_factor\n\n        # Modulation gate\n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n\n        # Candidate state\n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n\n        # Simplified output head\n        self.return_layer = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        batch_size, seq_length, _ = x.size()\n        h = torch.zeros(batch_size, self.hidden_dim, device=x.device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=x.device)\n\n        for t in range(seq_length):\n            x_t = x[:, t, :]\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            c = m_t * c + (1 - m_t) * candidate\n            h = torch.tanh(c)\n\n        ret = torch.tanh(self.return_layer(h)) * self.scale_factor\n        last_close = x[:, -1, 0].unsqueeze(1)\n        return last_close * (1 + ret)\n\n# 8. Training & Early Stopping\ndef train_model(model, train_loader, val_loader,\n                lr=1e-3, epochs=50, patience=5):\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    best_val = float('inf')\n    best_state = None\n    counter = 0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses = []\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss.item())\n\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                val_losses.append(criterion(model(Xb), yb).item())\n\n        avg_train = np.mean(train_losses)\n        avg_val   = np.mean(val_losses)\n        print(f'Epoch {epoch}: Train={avg_train:.6f}, Val={avg_val:.6f}')\n\n        if avg_val < best_val:\n            best_val = avg_val\n            best_state = model.state_dict()\n            counter = 0\n        else:\n            counter += 1\n            if counter >= patience:\n                print('Early stopping')\n                break\n\n    model.load_state_dict(best_state)\n    return model\n\n# 9. Evaluation\ndef evaluate(model, loader):\n    model.eval()\n    preds, trues = [], []\n    with torch.no_grad():\n        for Xb, yb in loader:\n            Xb = Xb.to(device)\n            out = model(Xb).cpu().numpy()\n            preds.append(out)\n            trues.append(yb.numpy())\n    preds = np.vstack(preds).flatten()\n    trues = np.vstack(trues).flatten()\n\n    def inv(x):\n        tmp = np.zeros((len(x), data.shape[1]))\n        tmp[:,0] = x\n        return scaler.inverse_transform(tmp)[:,0]\n\n    p_inv, t_inv = inv(preds), inv(trues)\n    return {\n        'R2': r2_score(t_inv, p_inv),\n        'MAE': mean_absolute_error(t_inv, p_inv),\n        'MSE': mean_squared_error(t_inv, p_inv),\n        'RMSE': np.sqrt(mean_squared_error(t_inv, p_inv)),\n        'MAPE': mean_absolute_percentage_error(t_inv, p_inv) * 100,\n        'EVS': explained_variance_score(t_inv, p_inv)\n    }, p_inv, t_inv\n\n# 10. Run weakened model\nmodel = LSTM_ModulationOnly_Weak(in_channels=data.shape[1]).to(device)\nmodel = train_model(model, train_loader, val_loader)\nmetrics, preds, actuals = evaluate(model, test_loader)\n\nprint('Weakened Modulation Only LSTM Test Metrics:')\nfor k, v in metrics.items():\n    print(f'  {k}: {v:.4f}')\n\n# 11. Plot\nplt.figure(figsize=(10, 4))\nplt.plot(actuals, label='Actual')\nplt.plot(preds, label='Predicted')\nplt.legend()\nplt.title('Weakened Modulation Only LSTM')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:20:02.904630Z","iopub.execute_input":"2025-06-14T13:20:02.905078Z","iopub.status.idle":"2025-06-14T13:20:09.138906Z","shell.execute_reply.started":"2025-06-14T13:20:02.905042Z","shell.execute_reply":"2025-06-14T13:20:09.137865Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final Model (StockTrendNet)","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# 1. Set Seeds for Reproducibility\n# -------------------------------\ndef set_random_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(30)\n\n# -------------------------------\n# 2. Load & Preprocess Data\n# -------------------------------\n# Make sure to load your DataFrame before this step.\n# For example: data = pd.read_csv('your_data.csv')\ndf = data.copy()  # 'data' should be defined\nall_cols = df.columns.tolist()\n\ntarget = \"Close\"\n# Ensure the target column is the first in the feature list.\nfeature_cols = [target] + [col for col in all_cols if col != target]\ndf = df[feature_cols].copy()\n\n# Convert numeric columns (remove commas if needed)\nfor col in feature_cols:\n    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\ndf = df.dropna()\n\n# -------------------------------\n# 3. Create Sequences from Time Series Data\n# -------------------------------\ndef create_sequences(data, seq_length=20, target_idx=0):\n    \"\"\"\n    Creates sequences from data.\n    Each sequence of length 'seq_length' has a target as the value at index target_idx\n    of the next timestep.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length, target_idx])  # target is \"Close\"\n    return np.array(X), np.array(y)\n\ndata_values = df.values  # All features as inputs\n\n# Split data into train (80%) and test (20%)\ntrain_size = int(len(data_values) * 0.8)\ntrain_data = data_values[:train_size]\ntest_data  = data_values[train_size:]\n\n# Scale the data using MinMaxScaler (applied on all features)\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train_data)\ntest_scaled  = scaler.transform(test_data)\n\n# Create sequences with a sequence length of 20\nseq_length = 20\nX_train, y_train = create_sequences(train_scaled, seq_length=seq_length, target_idx=0)\nX_test, y_test   = create_sequences(test_scaled, seq_length=seq_length, target_idx=0)\n\n# Further split training data into train and validation (e.g., 87.5% train, 12.5% validation)\ntrain_split = int(len(X_train) * 0.875)\nX_val = X_train[train_split:]\ny_val = y_train[train_split:]\nX_train = X_train[:train_split]\ny_train = y_train[:train_split]\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Validation shape:\", X_val.shape)\nprint(\"Test shape:\", X_test.shape)\n\n# -------------------------------\n# 4. Prepare PyTorch Datasets & DataLoaders\n# -------------------------------\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset   = TensorDataset(X_val_t, y_val_t)\ntest_dataset  = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# -------------------------------\n# 5. Define Merged Custom LSTM Model\n# -------------------------------\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, in_channels, seq_length=20, hidden_dim=64, dense_dim=32, scale_factor=0.2):\n        \"\"\"\n        A custom LSTM model that internally uses its own LSTM cell logic.\n        Processes the input sequence by iterating over every time step.\n        \"\"\"\n        super(CustomLSTMModel, self).__init__()\n        self.seq_length = seq_length\n        self.scale_factor = scale_factor\n        self.hidden_dim = hidden_dim\n        \n        # LSTM cell parameters (merged from CustomLSTMCell)\n        self.W_z = nn.Linear(in_channels, hidden_dim)\n        self.U_z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_m = nn.Linear(in_channels, hidden_dim)\n        self.U_m = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_m = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.W_c = nn.Linear(in_channels, hidden_dim)\n        self.U_c = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Residual connection: project previous hidden state\n        self.residual_linear = nn.Linear(hidden_dim, hidden_dim)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Skip connection: project the input to hidden_dim\n        self.input_linear = nn.Linear(in_channels, hidden_dim)\n        \n        # Dense layers for output processing\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_dim, dense_dim),\n            nn.ReLU()\n        )\n        self.return_layer = nn.Linear(dense_dim, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_length, in_channels)\n        batch_size = x.size(0)\n        device = x.device\n        \n        # Initialize hidden and cell states to zeros\n        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        # Process the sequence one timestep at a time\n        for t in range(self.seq_length):\n            x_t = x[:, t, :]\n            z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h) + self.b_z)\n            m_t = torch.sigmoid(self.W_m(x_t) + self.U_m(h) + self.b_m)\n            candidate = torch.tanh(self.W_c(x_t) + self.U_c(h) + self.b_c)\n            candidate = candidate + self.residual_linear(h)\n            candidate = candidate + self.input_linear(x_t)\n            candidate = self.layer_norm(candidate)\n            \n            c = (1 - z_t) * c + z_t * candidate\n            h = m_t * torch.tanh(c)\n        \n        # Compute dense layers and output\n        dense_out = self.dense(h)\n        long_term_return = torch.tanh(self.return_layer(dense_out)) * self.scale_factor\n        \n        # Use the last day's \"Close\" price from the input sequence (assumes first feature is \"Close\")\n        last_day = x[:, -1, 0].unsqueeze(1)\n        predicted_price = last_day * (1 + long_term_return)\n        return predicted_price\n\n# -------------------------------\n# 6. Training and Evaluation Pipeline\n# -------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nin_channels = len(feature_cols)\nmodel = CustomLSTMModel(in_channels, seq_length=seq_length, hidden_dim=64, dense_dim=32, scale_factor=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    for batch_X, batch_y in train_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n    \n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            val_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(train_losses)\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\nmodel.load_state_dict(best_model_state)\n\n# -------------------------------\n# 7. Evaluation on Test Data\n# -------------------------------\nmodel.eval()\npredictions = []\ntrue_values = []\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        preds = model(batch_X)\n        predictions.append(preds.cpu().numpy())\n        true_values.append(batch_y.cpu().numpy())\n\npredictions = np.vstack(predictions)\ntrue_values = np.vstack(true_values)\n\ndef inverse_transform(values):\n    dummy = np.zeros((len(values), len(feature_cols)))\n    dummy[:, 0] = values.flatten()\n    return scaler.inverse_transform(dummy)[:, 0]\n\npredictions_inv = inverse_transform(predictions)\ntrue_values_inv = inverse_transform(true_values)\n\nr2 = r2_score(true_values_inv, predictions_inv)\nmae = mean_absolute_error(true_values_inv, predictions_inv)\nmse = mean_squared_error(true_values_inv, predictions_inv)\nrmse = np.sqrt(mse)\nevs = explained_variance_score(true_values_inv, predictions_inv)\nmape = mean_absolute_percentage_error(true_values_inv, predictions_inv) * 100\n\nr2_scaled = r2_score(true_values, predictions)\nmae_scaled = mean_absolute_error(true_values, predictions)\nmse_scaled = mean_squared_error(true_values, predictions)\nrmse_scaled = np.sqrt(mse_scaled)\nevs_scaled = explained_variance_score(true_values, predictions)\nmape_scaled = mean_absolute_percentage_error(true_values, predictions) * 100\n\nprint(\"ðŸ“Š Final Results -\")\nprint(f\"  RÂ²: {r2:.4f}\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  RMSE: {rmse:.4f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  EVS: {evs:.4f}\")\n\nprint(\"\\n--- Scaled Metrics ---\")\nprint(f\"RÂ² Score: {r2_scaled:.4f}\")\nprint(f\"MAE: {mae_scaled:.4f}\")\nprint(f\"MSE: {mse_scaled:.4f}\")\nprint(f\"RMSE: {rmse_scaled:.4f}\")\nprint(f\"Explained Variance Score: {evs_scaled:.4f}\")\nprint(f\"MAPE: {mape_scaled:.2f}%\")\n\nplt.figure(figsize=(10,5))\nplt.plot(true_values_inv, label='Actual')\nplt.plot(predictions_inv, label='Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Close Price\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T13:22:01.026822Z","iopub.execute_input":"2025-06-14T13:22:01.027261Z","iopub.status.idle":"2025-06-14T13:22:32.393522Z","shell.execute_reply.started":"2025-06-14T13:22:01.027230Z","shell.execute_reply":"2025-06-14T13:22:32.392452Z"}},"outputs":[],"execution_count":null}]}